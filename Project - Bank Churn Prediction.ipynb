{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymaUL53U1OtT"
   },
   "source": [
    "## Project - Bank Churn Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UHlpoeq51Prk",
    "outputId": "6267c8d4-9112-4c2e-e6dd-8fe9930804e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "#Importing Tensorflow\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "GgJL7O041WA9"
   },
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, auc\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuGuEWMVP9iZ"
   },
   "source": [
    "####1. Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "rwHlKPvp1Zkk"
   },
   "outputs": [],
   "source": [
    "#Mounting Google Drive\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NmGq6d9h1cej",
    "outputId": "8ffc7fe2-8956-4e5a-d2a2-4af27b59821c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "73lW6Tbv1fer"
   },
   "outputs": [],
   "source": [
    "project_path = '/content/drive/My Drive/Colab Notebooks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "SKPl_O0K1ouT"
   },
   "outputs": [],
   "source": [
    "dataset = project_path + 'bank.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "kbaGBA6G2SNT"
   },
   "outputs": [],
   "source": [
    "dframe = pd.read_csv(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "BSrcr9t82mNq",
    "outputId": "329204a1-695b-4a37-8d92-e5ff890806f1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  ...  IsActiveMember EstimatedSalary Exited\n",
       "0          1    15634602  Hargrave  ...               1       101348.88      1\n",
       "1          2    15647311      Hill  ...               1       112542.58      0\n",
       "2          3    15619304      Onio  ...               0       113931.57      1\n",
       "3          4    15701354      Boni  ...               0        93826.63      0\n",
       "4          5    15737888  Mitchell  ...               1        79084.10      0\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-fCBxN7QLOl"
   },
   "source": [
    "####2. Drop the columns which are unique for all users like IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "QGnmCaaf4Sdl"
   },
   "outputs": [],
   "source": [
    "#2. Drop the columns which are unique for all users like IDs\n",
    "dframe = dframe.drop([\"RowNumber\",\"CustomerId\",\"Surname\", \"Geography\",\t\"Gender\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "JTtdNDaGCUEC",
    "outputId": "4b6a785f-a574-4f44-8a2b-a9f440652107"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore  Age  Tenure  ...  IsActiveMember  EstimatedSalary  Exited\n",
       "0          619   42       2  ...               1        101348.88       1\n",
       "1          608   41       1  ...               1        112542.58       0\n",
       "2          502   42       8  ...               0        113931.57       1\n",
       "3          699   39       1  ...               0         93826.63       0\n",
       "4          850   43       2  ...               1         79084.10       0\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking new dataframe\n",
    "dframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "bXQJxzRWH1Re",
    "outputId": "f7dc1a5e-9917-4f95-a123-4077c53bbe2f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>650.528800</td>\n",
       "      <td>38.921800</td>\n",
       "      <td>5.012800</td>\n",
       "      <td>76485.889288</td>\n",
       "      <td>1.530200</td>\n",
       "      <td>0.70550</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>100090.239881</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>96.653299</td>\n",
       "      <td>10.487806</td>\n",
       "      <td>2.892174</td>\n",
       "      <td>62397.405202</td>\n",
       "      <td>0.581654</td>\n",
       "      <td>0.45584</td>\n",
       "      <td>0.499797</td>\n",
       "      <td>57510.492818</td>\n",
       "      <td>0.402769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>350.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.580000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>584.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51002.110000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>652.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>97198.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100193.915000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>718.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>127644.240000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>149388.247500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>850.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>250898.090000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>199992.480000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CreditScore           Age  ...  EstimatedSalary        Exited\n",
       "count  10000.000000  10000.000000  ...     10000.000000  10000.000000\n",
       "mean     650.528800     38.921800  ...    100090.239881      0.203700\n",
       "std       96.653299     10.487806  ...     57510.492818      0.402769\n",
       "min      350.000000     18.000000  ...        11.580000      0.000000\n",
       "25%      584.000000     32.000000  ...     51002.110000      0.000000\n",
       "50%      652.000000     37.000000  ...    100193.915000      0.000000\n",
       "75%      718.000000     44.000000  ...    149388.247500      0.000000\n",
       "max      850.000000     92.000000  ...    199992.480000      1.000000\n",
       "\n",
       "[8 rows x 9 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xN36BXOVQPZ5"
   },
   "source": [
    "####3. Distinguish the feature and target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "hGeAO3AuCaz4"
   },
   "outputs": [],
   "source": [
    "# Specify the data \n",
    "X=dframe.iloc[:,4:7]\n",
    "\n",
    "# Specifying target labels\n",
    "y = dframe.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUFrsRGvQUpx"
   },
   "source": [
    "####4. Divide the data set into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "UHDkrB3PKzZR"
   },
   "outputs": [],
   "source": [
    "#4. Divide the data set into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "y_train =  np.array(y_train)\n",
    "y_test =  np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLFsA95aQapZ"
   },
   "source": [
    "####5. Normalize the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "zTFn3QWQXr6Q"
   },
   "outputs": [],
   "source": [
    "X=tf.keras.utils.normalize(\n",
    "    X, axis=-1, order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BxVOCbPCv14i",
    "outputId": "074be323-77ec-4e92-b73d-deec59f8e57a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 3)\n",
      "(2000, 3)\n",
      "(8000,)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7dL5TfoQfCp"
   },
   "source": [
    "####6. Initialize & build the model. Identify the points of improvement and implement the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "NH8Pp55R0vpH"
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "ER4zhAkjTJ-U"
   },
   "outputs": [],
   "source": [
    "model.add(Dense(32, input_shape = (32,3), activation = 'relu'))\n",
    "model.add(Dense(32, activation = 'tanh'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzutHHXvQpHc"
   },
   "source": [
    "###7. Predict the results using 0.5 as a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "88_rkE2v09nu"
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.Adam(lr = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "Ap5CvqQ92A1v"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = sgd, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Rvp32oDVUFJ",
    "outputId": "162dc144-0822-46da-9ad5-ffb327d8b5f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 32, 32)            128       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32, 32)            1056      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32, 1)             33        \n",
      "=================================================================\n",
      "Total params: 1,217\n",
      "Trainable params: 1,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loojLPb3Qy77"
   },
   "source": [
    "######To begin we will test the model with batch size=10 and epochs=10 and verify the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3P_DS2ps1Cwp",
    "outputId": "a1939088-e975-417c-ec85-c8bdfe7408f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 32, 3) for input Tensor(\"dense_3_input:0\", shape=(None, 32, 3), dtype=float32), but it was called on an input with incompatible shape (10, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 32, 3) for input Tensor(\"dense_3_input:0\", shape=(None, 32, 3), dtype=float32), but it was called on an input with incompatible shape (10, 3).\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.9926 - accuracy: 0.6977\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.9179 - accuracy: 0.6967\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.9564 - accuracy: 0.6940\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.8136 - accuracy: 0.7075\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.9329 - accuracy: 0.6980\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.9045 - accuracy: 0.7088\n",
      "Epoch 7/10\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 1.0075 - accuracy: 0.6988\n",
      "Epoch 8/10\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.9388 - accuracy: 0.7030\n",
      "Epoch 9/10\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.8605 - accuracy: 0.7078\n",
      "Epoch 10/10\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.9436 - accuracy: 0.6995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f00c9e506d8>"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing the model with batch_size = 10 and epochs = 10\n",
    "model.fit(X_train, y_train, batch_size = 10, epochs = 10, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFXALLngQ5Wa"
   },
   "source": [
    "###### We could see an accuracy for around 70% or 71%, we will now increase the number of batch size and epochs to see if there is an improvement on the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dTWzNYC6ME6_",
    "outputId": "96f38b81-72c6-4356-e401-fbdc2cf715ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 32, 3) for input Tensor(\"dense_3_input:0\", shape=(None, 32, 3), dtype=float32), but it was called on an input with incompatible shape (50, 3).\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6004 - accuracy: 0.7638\n",
      "Epoch 2/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 0.5687 - accuracy: 0.7770\n",
      "Epoch 3/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 0.6048 - accuracy: 0.7520\n",
      "Epoch 4/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5957 - accuracy: 0.7560\n",
      "Epoch 5/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6341 - accuracy: 0.7523\n",
      "Epoch 6/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 0.6698 - accuracy: 0.7412\n",
      "Epoch 7/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 0.5677 - accuracy: 0.7775\n",
      "Epoch 8/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6027 - accuracy: 0.7458\n",
      "Epoch 9/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6495 - accuracy: 0.7490\n",
      "Epoch 10/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6216 - accuracy: 0.7455\n",
      "Epoch 11/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5955 - accuracy: 0.7680\n",
      "Epoch 12/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6120 - accuracy: 0.7552\n",
      "Epoch 13/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5960 - accuracy: 0.7685\n",
      "Epoch 14/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6601 - accuracy: 0.7427\n",
      "Epoch 15/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6146 - accuracy: 0.7420\n",
      "Epoch 16/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5845 - accuracy: 0.7550\n",
      "Epoch 17/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6681 - accuracy: 0.7340\n",
      "Epoch 18/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 0.6658 - accuracy: 0.7617\n",
      "Epoch 19/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6510 - accuracy: 0.7533\n",
      "Epoch 20/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7542 - accuracy: 0.7318\n",
      "Epoch 21/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5958 - accuracy: 0.7663\n",
      "Epoch 22/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6264 - accuracy: 0.7370\n",
      "Epoch 23/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6012 - accuracy: 0.7582\n",
      "Epoch 24/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6198 - accuracy: 0.7480\n",
      "Epoch 25/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6471 - accuracy: 0.7480\n",
      "Epoch 26/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6555 - accuracy: 0.7308\n",
      "Epoch 27/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6638 - accuracy: 0.7278\n",
      "Epoch 28/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5855 - accuracy: 0.7695\n",
      "Epoch 29/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6011 - accuracy: 0.7502\n",
      "Epoch 30/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6243 - accuracy: 0.7640\n",
      "Epoch 31/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6771 - accuracy: 0.7290\n",
      "Epoch 32/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6782 - accuracy: 0.7470\n",
      "Epoch 33/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7078 - accuracy: 0.7377\n",
      "Epoch 34/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 0.6895 - accuracy: 0.7205\n",
      "Epoch 35/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6445 - accuracy: 0.7300\n",
      "Epoch 36/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6133 - accuracy: 0.7427\n",
      "Epoch 37/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5901 - accuracy: 0.7582\n",
      "Epoch 38/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6466 - accuracy: 0.7582\n",
      "Epoch 39/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7033 - accuracy: 0.7163\n",
      "Epoch 40/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5670 - accuracy: 0.7772\n",
      "Epoch 41/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6573 - accuracy: 0.7312\n",
      "Epoch 42/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6529 - accuracy: 0.7448\n",
      "Epoch 43/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6436 - accuracy: 0.7318\n",
      "Epoch 44/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6826 - accuracy: 0.7370\n",
      "Epoch 45/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6626 - accuracy: 0.7515\n",
      "Epoch 46/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6407 - accuracy: 0.7418\n",
      "Epoch 47/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6286 - accuracy: 0.7535\n",
      "Epoch 48/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6444 - accuracy: 0.7175\n",
      "Epoch 49/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6865 - accuracy: 0.7448\n",
      "Epoch 50/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6056 - accuracy: 0.7675\n",
      "Epoch 51/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7293 - accuracy: 0.7303\n",
      "Epoch 52/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6412 - accuracy: 0.7365\n",
      "Epoch 53/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6476 - accuracy: 0.7423\n",
      "Epoch 54/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5996 - accuracy: 0.7582\n",
      "Epoch 55/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6773 - accuracy: 0.7195\n",
      "Epoch 56/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7342 - accuracy: 0.7283\n",
      "Epoch 57/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5969 - accuracy: 0.7602\n",
      "Epoch 58/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6198 - accuracy: 0.7350\n",
      "Epoch 59/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5872 - accuracy: 0.7613\n",
      "Epoch 60/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6083 - accuracy: 0.7730\n",
      "Epoch 61/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6877 - accuracy: 0.7247\n",
      "Epoch 62/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5988 - accuracy: 0.7665\n",
      "Epoch 63/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6429 - accuracy: 0.7423\n",
      "Epoch 64/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7413 - accuracy: 0.7280\n",
      "Epoch 65/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6594 - accuracy: 0.7400\n",
      "Epoch 66/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6386 - accuracy: 0.7475\n",
      "Epoch 67/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6634 - accuracy: 0.7318\n",
      "Epoch 68/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6720 - accuracy: 0.7520\n",
      "Epoch 69/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6057 - accuracy: 0.7585\n",
      "Epoch 70/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6885 - accuracy: 0.7442\n",
      "Epoch 71/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5605 - accuracy: 0.7772\n",
      "Epoch 72/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5947 - accuracy: 0.7648\n",
      "Epoch 73/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6288 - accuracy: 0.7410\n",
      "Epoch 74/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6298 - accuracy: 0.7555\n",
      "Epoch 75/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6266 - accuracy: 0.7375\n",
      "Epoch 76/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6645 - accuracy: 0.7300\n",
      "Epoch 77/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6899 - accuracy: 0.7262\n",
      "Epoch 78/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6269 - accuracy: 0.7600\n",
      "Epoch 79/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6922 - accuracy: 0.7327\n",
      "Epoch 80/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6700 - accuracy: 0.7380\n",
      "Epoch 81/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6402 - accuracy: 0.7435\n",
      "Epoch 82/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8263 - accuracy: 0.7015\n",
      "Epoch 83/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6024 - accuracy: 0.7600\n",
      "Epoch 84/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5740 - accuracy: 0.7717\n",
      "Epoch 85/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6042 - accuracy: 0.7492\n",
      "Epoch 86/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6575 - accuracy: 0.7268\n",
      "Epoch 87/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6169 - accuracy: 0.7540\n",
      "Epoch 88/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5988 - accuracy: 0.7638\n",
      "Epoch 89/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 0.7199 - accuracy: 0.7178\n",
      "Epoch 90/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7245 - accuracy: 0.7097\n",
      "Epoch 91/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5892 - accuracy: 0.7650\n",
      "Epoch 92/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6314 - accuracy: 0.7295\n",
      "Epoch 93/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5879 - accuracy: 0.7580\n",
      "Epoch 94/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6418 - accuracy: 0.7510\n",
      "Epoch 95/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6612 - accuracy: 0.7318\n",
      "Epoch 96/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 0.6610 - accuracy: 0.7170\n",
      "Epoch 97/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6113 - accuracy: 0.7515\n",
      "Epoch 98/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6480 - accuracy: 0.7415\n",
      "Epoch 99/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6593 - accuracy: 0.7390\n",
      "Epoch 100/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6159 - accuracy: 0.7470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f00c9603f98>"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing the model with batch_size = 50 and epochs = 100\n",
    "model.fit(X_train, y_train, batch_size = 50, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwyzPwfkRDiq"
   },
   "source": [
    "###### We can see now an improvement after incresing to batch size=50 and epochs=100. The accuracy of the model is now around 75% We will now increase the batch size and epochs to see if there is a new increase in the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zMpah38tMUfz",
    "outputId": "0a4def53-6abf-4a90-a238-d75b6179df66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 32, 3) for input Tensor(\"dense_3_input:0\", shape=(None, 32, 3), dtype=float32), but it was called on an input with incompatible shape (None, 3).\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5563 - accuracy: 0.7715\n",
      "Epoch 2/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5437 - accuracy: 0.7945\n",
      "Epoch 3/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5452 - accuracy: 0.7880\n",
      "Epoch 4/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5855 - accuracy: 0.7670\n",
      "Epoch 5/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5391 - accuracy: 0.7945\n",
      "Epoch 6/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5779 - accuracy: 0.7645\n",
      "Epoch 7/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6059 - accuracy: 0.7515\n",
      "Epoch 8/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6113 - accuracy: 0.7615\n",
      "Epoch 9/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6565 - accuracy: 0.7372\n",
      "Epoch 10/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6957 - accuracy: 0.7310\n",
      "Epoch 11/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6160 - accuracy: 0.7510\n",
      "Epoch 12/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6562 - accuracy: 0.7423\n",
      "Epoch 13/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5899 - accuracy: 0.7793\n",
      "Epoch 14/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5784 - accuracy: 0.7530\n",
      "Epoch 15/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5716 - accuracy: 0.7653\n",
      "Epoch 16/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6403 - accuracy: 0.7455\n",
      "Epoch 17/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6230 - accuracy: 0.7613\n",
      "Epoch 18/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5693 - accuracy: 0.7790\n",
      "Epoch 19/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6594 - accuracy: 0.7230\n",
      "Epoch 20/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6234 - accuracy: 0.7345\n",
      "Epoch 21/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5561 - accuracy: 0.7732\n",
      "Epoch 22/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5724 - accuracy: 0.7592\n",
      "Epoch 23/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6144 - accuracy: 0.7632\n",
      "Epoch 24/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6023 - accuracy: 0.7515\n",
      "Epoch 25/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5424 - accuracy: 0.7862\n",
      "Epoch 26/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5355 - accuracy: 0.7945\n",
      "Epoch 27/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5443 - accuracy: 0.7862\n",
      "Epoch 28/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6020 - accuracy: 0.7550\n",
      "Epoch 29/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5821 - accuracy: 0.7590\n",
      "Epoch 30/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6099 - accuracy: 0.7502\n",
      "Epoch 31/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5738 - accuracy: 0.7795\n",
      "Epoch 32/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5569 - accuracy: 0.7753\n",
      "Epoch 33/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7048 - accuracy: 0.7138\n",
      "Epoch 34/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5891 - accuracy: 0.7545\n",
      "Epoch 35/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5564 - accuracy: 0.7875\n",
      "Epoch 36/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5724 - accuracy: 0.7820\n",
      "Epoch 37/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6003 - accuracy: 0.7585\n",
      "Epoch 38/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6656 - accuracy: 0.7333\n",
      "Epoch 39/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5955 - accuracy: 0.7648\n",
      "Epoch 40/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6700 - accuracy: 0.7097\n",
      "Epoch 41/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6215 - accuracy: 0.7247\n",
      "Epoch 42/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5494 - accuracy: 0.7945\n",
      "Epoch 43/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5987 - accuracy: 0.7477\n",
      "Epoch 44/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5618 - accuracy: 0.7625\n",
      "Epoch 45/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5692 - accuracy: 0.7832\n",
      "Epoch 46/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5935 - accuracy: 0.7615\n",
      "Epoch 47/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5934 - accuracy: 0.7498\n",
      "Epoch 48/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5521 - accuracy: 0.7735\n",
      "Epoch 49/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6804 - accuracy: 0.7498\n",
      "Epoch 50/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5904 - accuracy: 0.7490\n",
      "Epoch 51/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6791 - accuracy: 0.7210\n",
      "Epoch 52/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5958 - accuracy: 0.7585\n",
      "Epoch 53/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5818 - accuracy: 0.7502\n",
      "Epoch 54/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5497 - accuracy: 0.7880\n",
      "Epoch 55/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5836 - accuracy: 0.7525\n",
      "Epoch 56/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5914 - accuracy: 0.7635\n",
      "Epoch 57/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6493 - accuracy: 0.7285\n",
      "Epoch 58/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5683 - accuracy: 0.7617\n",
      "Epoch 59/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6348 - accuracy: 0.7195\n",
      "Epoch 60/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6366 - accuracy: 0.7268\n",
      "Epoch 61/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5639 - accuracy: 0.7945\n",
      "Epoch 62/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5487 - accuracy: 0.7800\n",
      "Epoch 63/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5911 - accuracy: 0.7517\n",
      "Epoch 64/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5739 - accuracy: 0.7590\n",
      "Epoch 65/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.8820 - accuracy: 0.7128\n",
      "Epoch 66/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5285 - accuracy: 0.7878\n",
      "Epoch 67/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5863 - accuracy: 0.7663\n",
      "Epoch 68/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6452 - accuracy: 0.7430\n",
      "Epoch 69/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6462 - accuracy: 0.7405\n",
      "Epoch 70/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5999 - accuracy: 0.7577\n",
      "Epoch 71/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5526 - accuracy: 0.7945\n",
      "Epoch 72/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6506 - accuracy: 0.7190\n",
      "Epoch 73/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5371 - accuracy: 0.7870\n",
      "Epoch 74/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6694 - accuracy: 0.7253\n",
      "Epoch 75/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7267 - accuracy: 0.6817\n",
      "Epoch 76/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6020 - accuracy: 0.7735\n",
      "Epoch 77/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6252 - accuracy: 0.7330\n",
      "Epoch 78/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5445 - accuracy: 0.7945\n",
      "Epoch 79/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5792 - accuracy: 0.7573\n",
      "Epoch 80/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5405 - accuracy: 0.7883\n",
      "Epoch 81/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6242 - accuracy: 0.7477\n",
      "Epoch 82/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5743 - accuracy: 0.7788\n",
      "Epoch 83/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6182 - accuracy: 0.7563\n",
      "Epoch 84/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6018 - accuracy: 0.7645\n",
      "Epoch 85/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5814 - accuracy: 0.7630\n",
      "Epoch 86/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6372 - accuracy: 0.7390\n",
      "Epoch 87/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5700 - accuracy: 0.7657\n",
      "Epoch 88/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5490 - accuracy: 0.7862\n",
      "Epoch 89/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5948 - accuracy: 0.7477\n",
      "Epoch 90/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5936 - accuracy: 0.7448\n",
      "Epoch 91/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5498 - accuracy: 0.7870\n",
      "Epoch 92/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5589 - accuracy: 0.7872\n",
      "Epoch 93/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6431 - accuracy: 0.7427\n",
      "Epoch 94/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6611 - accuracy: 0.7345\n",
      "Epoch 95/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5920 - accuracy: 0.7722\n",
      "Epoch 96/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6219 - accuracy: 0.7657\n",
      "Epoch 97/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6309 - accuracy: 0.7303\n",
      "Epoch 98/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5592 - accuracy: 0.7878\n",
      "Epoch 99/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5490 - accuracy: 0.7735\n",
      "Epoch 100/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5659 - accuracy: 0.7788\n",
      "Epoch 101/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6098 - accuracy: 0.7450\n",
      "Epoch 102/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6007 - accuracy: 0.7700\n",
      "Epoch 103/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6334 - accuracy: 0.7360\n",
      "Epoch 104/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5641 - accuracy: 0.7580\n",
      "Epoch 105/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5503 - accuracy: 0.7872\n",
      "Epoch 106/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6221 - accuracy: 0.7415\n",
      "Epoch 107/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6902 - accuracy: 0.7402\n",
      "Epoch 108/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5585 - accuracy: 0.7788\n",
      "Epoch 109/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5775 - accuracy: 0.7592\n",
      "Epoch 110/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6137 - accuracy: 0.7385\n",
      "Epoch 111/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6215 - accuracy: 0.7330\n",
      "Epoch 112/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5674 - accuracy: 0.7750\n",
      "Epoch 113/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5520 - accuracy: 0.7870\n",
      "Epoch 114/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7648 - accuracy: 0.6898\n",
      "Epoch 115/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5281 - accuracy: 0.7945\n",
      "Epoch 116/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6032 - accuracy: 0.7487\n",
      "Epoch 117/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6124 - accuracy: 0.7590\n",
      "Epoch 118/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5890 - accuracy: 0.7700\n",
      "Epoch 119/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6274 - accuracy: 0.7415\n",
      "Epoch 120/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5592 - accuracy: 0.7785\n",
      "Epoch 121/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6555 - accuracy: 0.7305\n",
      "Epoch 122/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6553 - accuracy: 0.7172\n",
      "Epoch 123/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6499 - accuracy: 0.7278\n",
      "Epoch 124/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6127 - accuracy: 0.7415\n",
      "Epoch 125/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5940 - accuracy: 0.7590\n",
      "Epoch 126/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6138 - accuracy: 0.7555\n",
      "Epoch 127/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5875 - accuracy: 0.7713\n",
      "Epoch 128/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6217 - accuracy: 0.7412\n",
      "Epoch 129/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6239 - accuracy: 0.7498\n",
      "Epoch 130/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5993 - accuracy: 0.7645\n",
      "Epoch 131/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5603 - accuracy: 0.7785\n",
      "Epoch 132/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6378 - accuracy: 0.7505\n",
      "Epoch 133/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6086 - accuracy: 0.7550\n",
      "Epoch 134/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5625 - accuracy: 0.7715\n",
      "Epoch 135/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5817 - accuracy: 0.7510\n",
      "Epoch 136/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5507 - accuracy: 0.7747\n",
      "Epoch 137/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5886 - accuracy: 0.7660\n",
      "Epoch 138/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5687 - accuracy: 0.7667\n",
      "Epoch 139/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6301 - accuracy: 0.7165\n",
      "Epoch 140/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5940 - accuracy: 0.7475\n",
      "Epoch 141/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6255 - accuracy: 0.7337\n",
      "Epoch 142/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5753 - accuracy: 0.7635\n",
      "Epoch 143/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5606 - accuracy: 0.7872\n",
      "Epoch 144/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6100 - accuracy: 0.7555\n",
      "Epoch 145/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5892 - accuracy: 0.7530\n",
      "Epoch 146/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6036 - accuracy: 0.7588\n",
      "Epoch 147/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5907 - accuracy: 0.7630\n",
      "Epoch 148/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5776 - accuracy: 0.7653\n",
      "Epoch 149/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5576 - accuracy: 0.7815\n",
      "Epoch 150/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5505 - accuracy: 0.7812\n",
      "Epoch 151/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6168 - accuracy: 0.7582\n",
      "Epoch 152/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6513 - accuracy: 0.7293\n",
      "Epoch 153/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5742 - accuracy: 0.7797\n",
      "Epoch 154/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6193 - accuracy: 0.7620\n",
      "Epoch 155/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6287 - accuracy: 0.7433\n",
      "Epoch 156/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5425 - accuracy: 0.7945\n",
      "Epoch 157/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6030 - accuracy: 0.7570\n",
      "Epoch 158/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6139 - accuracy: 0.7517\n",
      "Epoch 159/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5930 - accuracy: 0.7505\n",
      "Epoch 160/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5888 - accuracy: 0.7558\n",
      "Epoch 161/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6412 - accuracy: 0.7280\n",
      "Epoch 162/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5577 - accuracy: 0.7868\n",
      "Epoch 163/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6514 - accuracy: 0.7450\n",
      "Epoch 164/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5958 - accuracy: 0.7530\n",
      "Epoch 165/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6177 - accuracy: 0.7490\n",
      "Epoch 166/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6983 - accuracy: 0.7160\n",
      "Epoch 167/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5986 - accuracy: 0.7615\n",
      "Epoch 168/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5577 - accuracy: 0.7715\n",
      "Epoch 169/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5978 - accuracy: 0.7707\n",
      "Epoch 170/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5680 - accuracy: 0.7703\n",
      "Epoch 171/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5794 - accuracy: 0.7573\n",
      "Epoch 172/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5428 - accuracy: 0.7862\n",
      "Epoch 173/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5712 - accuracy: 0.7707\n",
      "Epoch 174/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6111 - accuracy: 0.7560\n",
      "Epoch 175/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6124 - accuracy: 0.7473\n",
      "Epoch 176/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5509 - accuracy: 0.7862\n",
      "Epoch 177/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5784 - accuracy: 0.7655\n",
      "Epoch 178/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5873 - accuracy: 0.7617\n",
      "Epoch 179/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6086 - accuracy: 0.7437\n",
      "Epoch 180/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6302 - accuracy: 0.7560\n",
      "Epoch 181/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.7398\n",
      "Epoch 182/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5408 - accuracy: 0.7945\n",
      "Epoch 183/500\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.5902 - accuracy: 0.7717\n",
      "Epoch 184/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5846 - accuracy: 0.7725\n",
      "Epoch 185/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6114 - accuracy: 0.7412\n",
      "Epoch 186/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5715 - accuracy: 0.7810\n",
      "Epoch 187/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6393 - accuracy: 0.7390\n",
      "Epoch 188/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6746 - accuracy: 0.7387\n",
      "Epoch 189/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5853 - accuracy: 0.7747\n",
      "Epoch 190/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6439 - accuracy: 0.7215\n",
      "Epoch 191/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5417 - accuracy: 0.7945\n",
      "Epoch 192/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5396 - accuracy: 0.7945\n",
      "Epoch 193/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6570 - accuracy: 0.7425\n",
      "Epoch 194/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6587 - accuracy: 0.7283\n",
      "Epoch 195/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6197 - accuracy: 0.7218\n",
      "Epoch 196/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6602 - accuracy: 0.7285\n",
      "Epoch 197/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6977 - accuracy: 0.7232\n",
      "Epoch 198/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5504 - accuracy: 0.7855\n",
      "Epoch 199/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6242 - accuracy: 0.7598\n",
      "Epoch 200/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6031 - accuracy: 0.7563\n",
      "Epoch 201/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5491 - accuracy: 0.7790\n",
      "Epoch 202/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5961 - accuracy: 0.7525\n",
      "Epoch 203/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5751 - accuracy: 0.7812\n",
      "Epoch 204/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5643 - accuracy: 0.7640\n",
      "Epoch 205/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6301 - accuracy: 0.7343\n",
      "Epoch 206/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6037 - accuracy: 0.7570\n",
      "Epoch 207/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6178 - accuracy: 0.7495\n",
      "Epoch 208/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6102 - accuracy: 0.7623\n",
      "Epoch 209/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5738 - accuracy: 0.7800\n",
      "Epoch 210/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5498 - accuracy: 0.7880\n",
      "Epoch 211/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5527 - accuracy: 0.7945\n",
      "Epoch 212/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6445 - accuracy: 0.7433\n",
      "Epoch 213/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5627 - accuracy: 0.7797\n",
      "Epoch 214/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5619 - accuracy: 0.7738\n",
      "Epoch 215/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5806 - accuracy: 0.7520\n",
      "Epoch 216/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5814 - accuracy: 0.7640\n",
      "Epoch 217/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5436 - accuracy: 0.7782\n",
      "Epoch 218/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5475 - accuracy: 0.7800\n",
      "Epoch 219/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6187 - accuracy: 0.7513\n",
      "Epoch 220/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6721 - accuracy: 0.7225\n",
      "Epoch 221/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5854 - accuracy: 0.7502\n",
      "Epoch 222/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5599 - accuracy: 0.7778\n",
      "Epoch 223/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5590 - accuracy: 0.7653\n",
      "Epoch 224/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6280 - accuracy: 0.7475\n",
      "Epoch 225/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5383 - accuracy: 0.7878\n",
      "Epoch 226/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5605 - accuracy: 0.7655\n",
      "Epoch 227/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6981 - accuracy: 0.7280\n",
      "Epoch 228/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5627 - accuracy: 0.7805\n",
      "Epoch 229/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5440 - accuracy: 0.7872\n",
      "Epoch 230/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5628 - accuracy: 0.7945\n",
      "Epoch 231/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5877 - accuracy: 0.7527\n",
      "Epoch 232/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5573 - accuracy: 0.7790\n",
      "Epoch 233/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6002 - accuracy: 0.7437\n",
      "Epoch 234/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.0258 - accuracy: 0.7210\n",
      "Epoch 235/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6537 - accuracy: 0.7398\n",
      "Epoch 236/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6368 - accuracy: 0.7483\n",
      "Epoch 237/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5629 - accuracy: 0.7740\n",
      "Epoch 238/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5733 - accuracy: 0.7657\n",
      "Epoch 239/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5540 - accuracy: 0.7945\n",
      "Epoch 240/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6137 - accuracy: 0.7660\n",
      "Epoch 241/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6340 - accuracy: 0.7372\n",
      "Epoch 242/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6189 - accuracy: 0.7430\n",
      "Epoch 243/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6331 - accuracy: 0.7372\n",
      "Epoch 244/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5789 - accuracy: 0.7705\n",
      "Epoch 245/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6101 - accuracy: 0.7595\n",
      "Epoch 246/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5981 - accuracy: 0.7533\n",
      "Epoch 247/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5984 - accuracy: 0.7655\n",
      "Epoch 248/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5868 - accuracy: 0.7722\n",
      "Epoch 249/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6014 - accuracy: 0.7630\n",
      "Epoch 250/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6474 - accuracy: 0.7283\n",
      "Epoch 251/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6298 - accuracy: 0.7325\n",
      "Epoch 252/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6541 - accuracy: 0.7225\n",
      "Epoch 253/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6121 - accuracy: 0.7445\n",
      "Epoch 254/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5955 - accuracy: 0.7655\n",
      "Epoch 255/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5914 - accuracy: 0.7640\n",
      "Epoch 256/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5843 - accuracy: 0.7642\n",
      "Epoch 257/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5572 - accuracy: 0.7945\n",
      "Epoch 258/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5493 - accuracy: 0.7865\n",
      "Epoch 259/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5634 - accuracy: 0.7795\n",
      "Epoch 260/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5580 - accuracy: 0.7887\n",
      "Epoch 261/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5606 - accuracy: 0.7885\n",
      "Epoch 262/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5435 - accuracy: 0.7945\n",
      "Epoch 263/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6351 - accuracy: 0.7343\n",
      "Epoch 264/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5628 - accuracy: 0.7717\n",
      "Epoch 265/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6618 - accuracy: 0.7530\n",
      "Epoch 266/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6686 - accuracy: 0.7157\n",
      "Epoch 267/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5826 - accuracy: 0.7640\n",
      "Epoch 268/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6262 - accuracy: 0.7585\n",
      "Epoch 269/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5795 - accuracy: 0.7860\n",
      "Epoch 270/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5931 - accuracy: 0.7473\n",
      "Epoch 271/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6059 - accuracy: 0.7508\n",
      "Epoch 272/500\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.5904 - accuracy: 0.7595\n",
      "Epoch 273/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6292 - accuracy: 0.7400\n",
      "Epoch 274/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5568 - accuracy: 0.7805\n",
      "Epoch 275/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5717 - accuracy: 0.7705\n",
      "Epoch 276/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5519 - accuracy: 0.7790\n",
      "Epoch 277/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5906 - accuracy: 0.7623\n",
      "Epoch 278/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6229 - accuracy: 0.7485\n",
      "Epoch 279/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5961 - accuracy: 0.7620\n",
      "Epoch 280/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7498 - accuracy: 0.7085\n",
      "Epoch 281/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5877 - accuracy: 0.7470\n",
      "Epoch 282/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6866 - accuracy: 0.6988\n",
      "Epoch 283/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6033 - accuracy: 0.7370\n",
      "Epoch 284/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5458 - accuracy: 0.7880\n",
      "Epoch 285/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6519 - accuracy: 0.7097\n",
      "Epoch 286/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5920 - accuracy: 0.7642\n",
      "Epoch 287/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6460 - accuracy: 0.7445\n",
      "Epoch 288/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5777 - accuracy: 0.7575\n",
      "Epoch 289/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6497 - accuracy: 0.7040\n",
      "Epoch 290/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5822 - accuracy: 0.7653\n",
      "Epoch 291/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5563 - accuracy: 0.7872\n",
      "Epoch 292/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6155 - accuracy: 0.7405\n",
      "Epoch 293/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6446 - accuracy: 0.7410\n",
      "Epoch 294/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5799 - accuracy: 0.7682\n",
      "Epoch 295/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6113 - accuracy: 0.7632\n",
      "Epoch 296/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5915 - accuracy: 0.7645\n",
      "Epoch 297/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6002 - accuracy: 0.7707\n",
      "Epoch 298/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5782 - accuracy: 0.7875\n",
      "Epoch 299/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5778 - accuracy: 0.7725\n",
      "Epoch 300/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5586 - accuracy: 0.7725\n",
      "Epoch 301/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5350 - accuracy: 0.7945\n",
      "Epoch 302/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5919 - accuracy: 0.7653\n",
      "Epoch 303/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5950 - accuracy: 0.7602\n",
      "Epoch 304/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7080 - accuracy: 0.7180\n",
      "Epoch 305/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5682 - accuracy: 0.7790\n",
      "Epoch 306/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5882 - accuracy: 0.7590\n",
      "Epoch 307/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5560 - accuracy: 0.7875\n",
      "Epoch 308/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6731 - accuracy: 0.7265\n",
      "Epoch 309/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6039 - accuracy: 0.7418\n",
      "Epoch 310/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5754 - accuracy: 0.7793\n",
      "Epoch 311/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5934 - accuracy: 0.7490\n",
      "Epoch 312/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5634 - accuracy: 0.7715\n",
      "Epoch 313/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5700 - accuracy: 0.7717\n",
      "Epoch 314/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6236 - accuracy: 0.7398\n",
      "Epoch 315/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5814 - accuracy: 0.7588\n",
      "Epoch 316/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5741 - accuracy: 0.7707\n",
      "Epoch 317/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5434 - accuracy: 0.7883\n",
      "Epoch 318/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5921 - accuracy: 0.7565\n",
      "Epoch 319/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6156 - accuracy: 0.7513\n",
      "Epoch 320/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6630 - accuracy: 0.7333\n",
      "Epoch 321/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5385 - accuracy: 0.7945\n",
      "Epoch 322/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6311 - accuracy: 0.7505\n",
      "Epoch 323/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6565 - accuracy: 0.7280\n",
      "Epoch 324/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5710 - accuracy: 0.7795\n",
      "Epoch 325/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6234 - accuracy: 0.7340\n",
      "Epoch 326/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5690 - accuracy: 0.7660\n",
      "Epoch 327/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5767 - accuracy: 0.7730\n",
      "Epoch 328/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5424 - accuracy: 0.7832\n",
      "Epoch 329/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6084 - accuracy: 0.7412\n",
      "Epoch 330/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5953 - accuracy: 0.7630\n",
      "Epoch 331/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5443 - accuracy: 0.7872\n",
      "Epoch 332/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5601 - accuracy: 0.7738\n",
      "Epoch 333/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5578 - accuracy: 0.7803\n",
      "Epoch 334/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5773 - accuracy: 0.7657\n",
      "Epoch 335/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6118 - accuracy: 0.7485\n",
      "Epoch 336/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5486 - accuracy: 0.7807\n",
      "Epoch 337/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7673 - accuracy: 0.6945\n",
      "Epoch 338/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5464 - accuracy: 0.7945\n",
      "Epoch 339/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5441 - accuracy: 0.7945\n",
      "Epoch 340/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6307 - accuracy: 0.7318\n",
      "Epoch 341/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6584 - accuracy: 0.7280\n",
      "Epoch 342/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6022 - accuracy: 0.7757\n",
      "Epoch 343/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6595 - accuracy: 0.7258\n",
      "Epoch 344/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6102 - accuracy: 0.7545\n",
      "Epoch 345/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7253 - accuracy: 0.7293\n",
      "Epoch 346/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6044 - accuracy: 0.7362\n",
      "Epoch 347/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7572 - accuracy: 0.7030\n",
      "Epoch 348/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6640 - accuracy: 0.7275\n",
      "Epoch 349/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5635 - accuracy: 0.7638\n",
      "Epoch 350/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5673 - accuracy: 0.7793\n",
      "Epoch 351/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5700 - accuracy: 0.7795\n",
      "Epoch 352/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6272 - accuracy: 0.7575\n",
      "Epoch 353/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6175 - accuracy: 0.7215\n",
      "Epoch 354/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6297 - accuracy: 0.7613\n",
      "Epoch 355/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5518 - accuracy: 0.7807\n",
      "Epoch 356/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5788 - accuracy: 0.7715\n",
      "Epoch 357/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5730 - accuracy: 0.7490\n",
      "Epoch 358/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6023 - accuracy: 0.7592\n",
      "Epoch 359/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6004 - accuracy: 0.7460\n",
      "Epoch 360/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5610 - accuracy: 0.7862\n",
      "Epoch 361/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5508 - accuracy: 0.7887\n",
      "Epoch 362/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5687 - accuracy: 0.7655\n",
      "Epoch 363/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5957 - accuracy: 0.7607\n",
      "Epoch 364/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5575 - accuracy: 0.7880\n",
      "Epoch 365/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5780 - accuracy: 0.7738\n",
      "Epoch 366/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6011 - accuracy: 0.7567\n",
      "Epoch 367/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6646 - accuracy: 0.7347\n",
      "Epoch 368/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6051 - accuracy: 0.7570\n",
      "Epoch 369/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6394 - accuracy: 0.7245\n",
      "Epoch 370/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5698 - accuracy: 0.7550\n",
      "Epoch 371/500\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.5420 - accuracy: 0.7945\n",
      "Epoch 372/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6221 - accuracy: 0.7580\n",
      "Epoch 373/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5969 - accuracy: 0.7523\n",
      "Epoch 374/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5773 - accuracy: 0.7575\n",
      "Epoch 375/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5351 - accuracy: 0.7945\n",
      "Epoch 376/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6011 - accuracy: 0.7505\n",
      "Epoch 377/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6184 - accuracy: 0.7545\n",
      "Epoch 378/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5785 - accuracy: 0.7660\n",
      "Epoch 379/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5688 - accuracy: 0.7660\n",
      "Epoch 380/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5959 - accuracy: 0.7600\n",
      "Epoch 381/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6013 - accuracy: 0.7527\n",
      "Epoch 382/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5738 - accuracy: 0.7782\n",
      "Epoch 383/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6241 - accuracy: 0.7327\n",
      "Epoch 384/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5684 - accuracy: 0.7805\n",
      "Epoch 385/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5891 - accuracy: 0.7640\n",
      "Epoch 386/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6101 - accuracy: 0.7333\n",
      "Epoch 387/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5744 - accuracy: 0.7695\n",
      "Epoch 388/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5664 - accuracy: 0.7872\n",
      "Epoch 389/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5824 - accuracy: 0.7710\n",
      "Epoch 390/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5724 - accuracy: 0.7878\n",
      "Epoch 391/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6535 - accuracy: 0.7380\n",
      "Epoch 392/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7236 - accuracy: 0.7023\n",
      "Epoch 393/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6253 - accuracy: 0.7262\n",
      "Epoch 394/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7810\n",
      "Epoch 395/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5862 - accuracy: 0.7545\n",
      "Epoch 396/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5540 - accuracy: 0.7945\n",
      "Epoch 397/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5465 - accuracy: 0.7872\n",
      "Epoch 398/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5827 - accuracy: 0.7732\n",
      "Epoch 399/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6251 - accuracy: 0.7410\n",
      "Epoch 400/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7372 - accuracy: 0.7278\n",
      "Epoch 401/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5381 - accuracy: 0.7878\n",
      "Epoch 402/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6589 - accuracy: 0.7297\n",
      "Epoch 403/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6635 - accuracy: 0.7157\n",
      "Epoch 404/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5486 - accuracy: 0.7872\n",
      "Epoch 405/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6806 - accuracy: 0.7340\n",
      "Epoch 406/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5880 - accuracy: 0.7705\n",
      "Epoch 407/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6207 - accuracy: 0.7445\n",
      "Epoch 408/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5866 - accuracy: 0.7527\n",
      "Epoch 409/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5802 - accuracy: 0.7642\n",
      "Epoch 410/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5735 - accuracy: 0.7650\n",
      "Epoch 411/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5826 - accuracy: 0.7785\n",
      "Epoch 412/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5576 - accuracy: 0.7747\n",
      "Epoch 413/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5745 - accuracy: 0.7775\n",
      "Epoch 414/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5622 - accuracy: 0.7790\n",
      "Epoch 415/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6405 - accuracy: 0.7163\n",
      "Epoch 416/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6414 - accuracy: 0.7402\n",
      "Epoch 417/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5989 - accuracy: 0.7523\n",
      "Epoch 418/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6016 - accuracy: 0.7602\n",
      "Epoch 419/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7932 - accuracy: 0.6800\n",
      "Epoch 420/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6072 - accuracy: 0.7415\n",
      "Epoch 421/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5579 - accuracy: 0.7780\n",
      "Epoch 422/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5925 - accuracy: 0.7765\n",
      "Epoch 423/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5606 - accuracy: 0.7803\n",
      "Epoch 424/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5928 - accuracy: 0.7642\n",
      "Epoch 425/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5515 - accuracy: 0.7880\n",
      "Epoch 426/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6171 - accuracy: 0.7487\n",
      "Epoch 427/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5780 - accuracy: 0.7650\n",
      "Epoch 428/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5738 - accuracy: 0.7872\n",
      "Epoch 429/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6157 - accuracy: 0.7368\n",
      "Epoch 430/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6032 - accuracy: 0.7448\n",
      "Epoch 431/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6766 - accuracy: 0.7107\n",
      "Epoch 432/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5465 - accuracy: 0.7805\n",
      "Epoch 433/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5789 - accuracy: 0.7630\n",
      "Epoch 434/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5403 - accuracy: 0.7875\n",
      "Epoch 435/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5551 - accuracy: 0.7803\n",
      "Epoch 436/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5785 - accuracy: 0.7715\n",
      "Epoch 437/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7020 - accuracy: 0.7070\n",
      "Epoch 438/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7576 - accuracy: 0.7450\n",
      "Epoch 439/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5626 - accuracy: 0.7648\n",
      "Epoch 440/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6092 - accuracy: 0.7355\n",
      "Epoch 441/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6249 - accuracy: 0.7352\n",
      "Epoch 442/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5998 - accuracy: 0.7492\n",
      "Epoch 443/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6243 - accuracy: 0.7448\n",
      "Epoch 444/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6458 - accuracy: 0.7337\n",
      "Epoch 445/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6183 - accuracy: 0.7385\n",
      "Epoch 446/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6472 - accuracy: 0.7362\n",
      "Epoch 447/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6199 - accuracy: 0.7275\n",
      "Epoch 448/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6675 - accuracy: 0.7433\n",
      "Epoch 449/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5583 - accuracy: 0.7875\n",
      "Epoch 450/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6287 - accuracy: 0.7352\n",
      "Epoch 451/500\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.5852 - accuracy: 0.7747\n",
      "Epoch 452/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5761 - accuracy: 0.7720\n",
      "Epoch 453/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6179 - accuracy: 0.7387\n",
      "Epoch 454/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5396 - accuracy: 0.7945\n",
      "Epoch 455/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5732 - accuracy: 0.7558\n",
      "Epoch 456/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6720 - accuracy: 0.7280\n",
      "Epoch 457/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5621 - accuracy: 0.7945\n",
      "Epoch 458/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6298 - accuracy: 0.7405\n",
      "Epoch 459/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5287 - accuracy: 0.7945\n",
      "Epoch 460/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5703 - accuracy: 0.7757\n",
      "Epoch 461/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5363 - accuracy: 0.7945\n",
      "Epoch 462/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7131 - accuracy: 0.7075\n",
      "Epoch 463/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6024 - accuracy: 0.7685\n",
      "Epoch 464/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7275 - accuracy: 0.7253\n",
      "Epoch 465/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5803 - accuracy: 0.7640\n",
      "Epoch 466/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5552 - accuracy: 0.7862\n",
      "Epoch 467/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5573 - accuracy: 0.7795\n",
      "Epoch 468/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6339 - accuracy: 0.7220\n",
      "Epoch 469/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5910 - accuracy: 0.7642\n",
      "Epoch 470/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5427 - accuracy: 0.7805\n",
      "Epoch 471/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6584 - accuracy: 0.7520\n",
      "Epoch 472/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6153 - accuracy: 0.7585\n",
      "Epoch 473/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5433 - accuracy: 0.7865\n",
      "Epoch 474/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6077 - accuracy: 0.7728\n",
      "Epoch 475/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5944 - accuracy: 0.7513\n",
      "Epoch 476/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6605 - accuracy: 0.7182\n",
      "Epoch 477/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5714 - accuracy: 0.7692\n",
      "Epoch 478/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5925 - accuracy: 0.7510\n",
      "Epoch 479/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6363 - accuracy: 0.7300\n",
      "Epoch 480/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6490 - accuracy: 0.7480\n",
      "Epoch 481/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5430 - accuracy: 0.7945\n",
      "Epoch 482/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6841 - accuracy: 0.7268\n",
      "Epoch 483/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6083 - accuracy: 0.7602\n",
      "Epoch 484/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5642 - accuracy: 0.7585\n",
      "Epoch 485/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6292 - accuracy: 0.7460\n",
      "Epoch 486/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6407 - accuracy: 0.7377\n",
      "Epoch 487/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5778 - accuracy: 0.7738\n",
      "Epoch 488/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5485 - accuracy: 0.7800\n",
      "Epoch 489/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5944 - accuracy: 0.7585\n",
      "Epoch 490/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5990 - accuracy: 0.7598\n",
      "Epoch 491/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5366 - accuracy: 0.7945\n",
      "Epoch 492/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6436 - accuracy: 0.7437\n",
      "Epoch 493/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6639 - accuracy: 0.7190\n",
      "Epoch 494/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5841 - accuracy: 0.7598\n",
      "Epoch 495/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6018 - accuracy: 0.7635\n",
      "Epoch 496/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6140 - accuracy: 0.7552\n",
      "Epoch 497/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5528 - accuracy: 0.7945\n",
      "Epoch 498/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6357 - accuracy: 0.7502\n",
      "Epoch 499/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6568 - accuracy: 0.7498\n",
      "Epoch 500/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5911 - accuracy: 0.7548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f00c8d7df98>"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing the model with batch_size = 100 and epochs = 500\n",
    "model.fit(X_train, y_train, batch_size = 100, epochs = 500, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4Rvk2VYRT_Z"
   },
   "source": [
    "##### It seems like there is no significant improvement in the model if we increase to batch size = 100 and epochs = 500, the accuracy remains at around 75%. We will now verify the accuracy of the model and print the confusion matrix. Hence, we will remain at batch size = 50 and epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o82CtZ2RS0XA",
    "outputId": "49a376c7-cddf-493e-acc3-7407248e733c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6677 - accuracy: 0.7170\n",
      "Epoch 2/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7099 - accuracy: 0.7145\n",
      "Epoch 3/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6203 - accuracy: 0.7582\n",
      "Epoch 4/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7075 - accuracy: 0.7220\n",
      "Epoch 5/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6403 - accuracy: 0.7415\n",
      "Epoch 6/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6310 - accuracy: 0.7490\n",
      "Epoch 7/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6528 - accuracy: 0.7260\n",
      "Epoch 8/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5914 - accuracy: 0.7728\n",
      "Epoch 9/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6792 - accuracy: 0.7250\n",
      "Epoch 10/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6106 - accuracy: 0.7565\n",
      "Epoch 11/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.7105\n",
      "Epoch 12/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5939 - accuracy: 0.7690\n",
      "Epoch 13/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6523 - accuracy: 0.7303\n",
      "Epoch 14/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6679 - accuracy: 0.7385\n",
      "Epoch 15/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7175 - accuracy: 0.7343\n",
      "Epoch 16/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5837 - accuracy: 0.7613\n",
      "Epoch 17/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7857 - accuracy: 0.7262\n",
      "Epoch 18/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6503 - accuracy: 0.7475\n",
      "Epoch 19/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6239 - accuracy: 0.7548\n",
      "Epoch 20/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6341 - accuracy: 0.7433\n",
      "Epoch 21/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6434 - accuracy: 0.7375\n",
      "Epoch 22/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5682 - accuracy: 0.7775\n",
      "Epoch 23/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6164 - accuracy: 0.7610\n",
      "Epoch 24/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7433 - accuracy: 0.7272\n",
      "Epoch 25/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6218 - accuracy: 0.7473\n",
      "Epoch 26/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5848 - accuracy: 0.7640\n",
      "Epoch 27/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6531 - accuracy: 0.7505\n",
      "Epoch 28/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7340 - accuracy: 0.7215\n",
      "Epoch 29/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6213 - accuracy: 0.7575\n",
      "Epoch 30/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6068 - accuracy: 0.7513\n",
      "Epoch 31/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6322 - accuracy: 0.7465\n",
      "Epoch 32/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6356 - accuracy: 0.7377\n",
      "Epoch 33/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6504 - accuracy: 0.7405\n",
      "Epoch 34/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6765 - accuracy: 0.7212\n",
      "Epoch 35/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6284 - accuracy: 0.7505\n",
      "Epoch 36/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6482 - accuracy: 0.7485\n",
      "Epoch 37/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5741 - accuracy: 0.7613\n",
      "Epoch 38/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6142 - accuracy: 0.7480\n",
      "Epoch 39/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5855 - accuracy: 0.7742\n",
      "Epoch 40/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9392 - accuracy: 0.7260\n",
      "Epoch 41/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6481 - accuracy: 0.7368\n",
      "Epoch 42/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6845 - accuracy: 0.7295\n",
      "Epoch 43/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7087 - accuracy: 0.7590\n",
      "Epoch 44/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6414 - accuracy: 0.7442\n",
      "Epoch 45/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6058 - accuracy: 0.7505\n",
      "Epoch 46/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6532 - accuracy: 0.7355\n",
      "Epoch 47/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7156 - accuracy: 0.7138\n",
      "Epoch 48/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6371 - accuracy: 0.7480\n",
      "Epoch 49/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5880 - accuracy: 0.7628\n",
      "Epoch 50/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6244 - accuracy: 0.7548\n",
      "Epoch 51/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6205 - accuracy: 0.7540\n",
      "Epoch 52/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6463 - accuracy: 0.7345\n",
      "Epoch 53/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5942 - accuracy: 0.7630\n",
      "Epoch 54/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5971 - accuracy: 0.7638\n",
      "Epoch 55/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6429 - accuracy: 0.7477\n",
      "Epoch 56/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7126 - accuracy: 0.7272\n",
      "Epoch 57/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6985 - accuracy: 0.7175\n",
      "Epoch 58/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6039 - accuracy: 0.7588\n",
      "Epoch 59/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6011 - accuracy: 0.7495\n",
      "Epoch 60/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5844 - accuracy: 0.7685\n",
      "Epoch 61/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6416 - accuracy: 0.7415\n",
      "Epoch 62/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6240 - accuracy: 0.7635\n",
      "Epoch 63/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5905 - accuracy: 0.7577\n",
      "Epoch 64/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6679 - accuracy: 0.7070\n",
      "Epoch 65/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6413 - accuracy: 0.7377\n",
      "Epoch 66/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5894 - accuracy: 0.7688\n",
      "Epoch 67/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6442 - accuracy: 0.7390\n",
      "Epoch 68/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6366 - accuracy: 0.7287\n",
      "Epoch 69/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5769 - accuracy: 0.7785\n",
      "Epoch 70/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7006 - accuracy: 0.7023\n",
      "Epoch 71/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5961 - accuracy: 0.7675\n",
      "Epoch 72/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6263 - accuracy: 0.7473\n",
      "Epoch 73/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6362 - accuracy: 0.7540\n",
      "Epoch 74/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5777 - accuracy: 0.7738\n",
      "Epoch 75/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7738 - accuracy: 0.6805\n",
      "Epoch 76/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.6223 - accuracy: 0.7427\n",
      "Epoch 77/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.6776 - accuracy: 0.7305\n",
      "Epoch 78/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6646 - accuracy: 0.7300\n",
      "Epoch 79/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.6025 - accuracy: 0.7515\n",
      "Epoch 80/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5689 - accuracy: 0.7715\n",
      "Epoch 81/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5877 - accuracy: 0.7510\n",
      "Epoch 82/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6034 - accuracy: 0.7563\n",
      "Epoch 83/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5759 - accuracy: 0.7592\n",
      "Epoch 84/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6539 - accuracy: 0.7222\n",
      "Epoch 85/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5827 - accuracy: 0.7675\n",
      "Epoch 86/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6400 - accuracy: 0.7437\n",
      "Epoch 87/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.6259 - accuracy: 0.7505\n",
      "Epoch 88/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7202 - accuracy: 0.7312\n",
      "Epoch 89/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6003 - accuracy: 0.7510\n",
      "Epoch 90/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6731 - accuracy: 0.7405\n",
      "Epoch 91/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6180 - accuracy: 0.7410\n",
      "Epoch 92/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6649 - accuracy: 0.7193\n",
      "Epoch 93/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6147 - accuracy: 0.7667\n",
      "Epoch 94/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5840 - accuracy: 0.7595\n",
      "Epoch 95/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9071 - accuracy: 0.6860\n",
      "Epoch 96/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.6357 - accuracy: 0.7458\n",
      "Epoch 97/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6936 - accuracy: 0.7210\n",
      "Epoch 98/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6330 - accuracy: 0.7625\n",
      "Epoch 99/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8582 - accuracy: 0.7122\n",
      "Epoch 100/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.6029 - accuracy: 0.7592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f00c960f2e8>"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing the model with batch_size = 50 and epochs = 100\n",
    "model.fit(X_train, y_train, batch_size = 50, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVlZREAyRprI"
   },
   "source": [
    "#####8. Print the Accuracy score and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2feVWGxXsAm",
    "outputId": "55d4a846-da53-467d-a51a-261458efe70f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 32, 3) for input Tensor(\"dense_3_input:0\", shape=(None, 32, 3), dtype=float32), but it was called on an input with incompatible shape (None, 3).\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4986 - accuracy: 0.8035\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLUr6WFN47-0",
    "outputId": "fa7808cf-aebf-43e9-f29f-ae11014b3b03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "[0.4986029267311096, 0.8034999966621399]\n"
     ]
    }
   ],
   "source": [
    "print(model.metrics_names)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ab1UaRIQ5u5V",
    "outputId": "87ef1441-064e-4daf-c58f-087b616a034c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 32, 3) for input Tensor(\"dense_3_input:0\", shape=(None, 32, 3), dtype=float32), but it was called on an input with incompatible shape (200, 3).\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4986 - accuracy: 0.8035\n",
      "Accuracy Model1 (Dropout): 0.8034999966621399\n",
      "Recall_score: 0.0\n",
      "Precision_score: 0.0\n",
      "F-score: 0.0\n",
      "Confusion Matrix: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1607,    0],\n",
       "       [ 393,    0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_cls = model.predict_classes(X_test, batch_size=200, verbose=1)\n",
    "\n",
    "Y_pred_cls = model.predict_classes(X_test, batch_size=200, verbose=0)\n",
    "print('Accuracy Model1 (Dropout): '+ str(model.evaluate(X_test,y_test)[1]))\n",
    "print('Recall_score: ' + str(recall_score(y_test,Y_pred_cls)))\n",
    "print('Precision_score: ' + str(precision_score(y_test, Y_pred_cls)))\n",
    "print('F-score: ' + str(f1_score(y_test,Y_pred_cls)))\n",
    "print('Confusion Matrix: ')\n",
    "confusion_matrix(y_test, Y_pred_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zH1mc_GHPDau"
   },
   "source": [
    "##**Conlusions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_H1gO6l4PQYe"
   },
   "source": [
    "With this model we can predict with an accuracy of around 80% the customers that will leave in the next 6 months.\n",
    "In general, the accuracy of the model remains at around 76% or 76% and sometimes even lower. A good set of parameters will be batch size = 50 and epochs = 100\n",
    "There is no reason to increase the number of batchs and epochs as the performance remains the same."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
